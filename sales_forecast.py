import pandas as pd
import requests
import time
from datetime import datetime
import numpy as np
from sklearn.metrics import mean_squared_error  # å¼•å…¥èª¤å·®è©•ä¼°å‡½å¼
from google.oauth2.service_account import Credentials
import gspread
from gspread_dataframe import set_with_dataframe

SHEET_ID = "1ufAI8OY64NKrpLS17qlYuOX5HWgGlCyZsgPRC-NfQJI"
SHEET_NAME = "sheet1"
MODEL_DESCRIPTION_SHEET_NAME = "model_description"

# === è¨­å®š Google Sheet ===
SHEET_ID = "1ufAI8OY64NKrpLS17qlYuOX5HWgGlCyZsgPRC-NfQJI"
SHEET_NAME = "sheet1"
REPORT_SHEET_NAME = "report"
NEW_SKU_SHEET_NAME = "new_skus"
MODEL_DESCRIPTION_SHEET_NAME = "model_description"

# === å¾ç’°å¢ƒè®Šæ•¸è®€å– Slack Webhook URL ===
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")

# === Functionï¼šç™¼é€ Slack é€šçŸ¥ ===
def send_slack_message(message):
    if not SLACK_WEBHOOK_URL:
        print("âš ï¸ Slack Webhook URL æœªè¨­ç½®ï¼Œç•¥éé€šçŸ¥ã€‚")
        return
    payload = {"text": message}
    try:
        requests.post(SLACK_WEBHOOK_URL, json=payload)
    except Exception as e:
        print(f"âš ï¸ Slack ç™¼é€å¤±æ•—ï¼š{e}")

# === Functionï¼šAPI é‡è©¦æ©Ÿåˆ¶ ===
def fetch_data_with_retry(url, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = requests.get(url)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"âš ï¸ API å¤±æ•— {attempt + 1}/{max_retries} æ¬¡: {e}")
            time.sleep(2)
    send_slack_message(f"ğŸš¨ [é æ¸¬ç³»çµ±] API è³‡æ–™å–å¾—å¤±æ•—ï¼š{url}")
    raise Exception(f"ğŸš¨ API å¤±æ•—è¶…é {max_retries} æ¬¡ï¼š{url}")

# === Functionï¼šæ¬„ä½é˜²å‘† ===
def ensure_columns(df, columns):
    for col in columns:
        if col not in df.columns:
            print(f"âš ï¸ DataFrame ç¼ºå°‘æ¬„ä½ï¼š{col}ï¼Œè‡ªå‹•è£œ 0")
            df[col] = 0
    return df

# === Functionï¼šå¯«å…¥ Google Sheet ===
def write_to_gsheet(df, sheet_id, sheet_name):
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds = Credentials.from_service_account_file("besparks-service-account.json", scopes=scope)
    client = gspread.authorize(creds)
    sheet = client.open_by_key(sheet_id)
    worksheet = sheet.worksheet(sheet_name)
    worksheet.clear()
    set_with_dataframe(worksheet, df)

# === ä¸»æµç¨‹ ===
try:
   sales_data = fetch_data_with_retry("https://api.besparks.co/api:074LNDs2/data/sales_history")
   product_data = fetch_data_with_retry("https://api.besparks.co/api:074LNDs2/data/product_info")


   sales_df = pd.DataFrame(sales_data)
   product_df = pd.DataFrame(product_data)
   forecast_df = pd.DataFrame()


   # æ•¸å€¼è½‰æ›
   sales_df['quantity_sold'] = pd.to_numeric(sales_df['quantity_sold'], errors='coerce')
   product_df['price'] = pd.to_numeric(product_df['price'], errors='coerce')
   product_df['gross_margin'] = pd.to_numeric(product_df['gross_margin'], errors='coerce')


   # å•†å“ç¯©é¸
   product_df['type'] = product_df['type'].astype(str).str.lower()
   product_df['is_tangible'] = product_df['is_tangible'].astype(str).str.lower() == 'true'
   product_df = product_df[
       (product_df['type'] == 'single') &
       (product_df['is_tangible']) &
       (product_df['status'] != 'archived') &
       (~product_df['product_line'].isin(['Accessories', 'Others', 'Packaging', 'Raw Materials', 'Promotion Bundle']))
   ].copy()


   sales_df['date'] = pd.to_datetime(sales_df['date'], format='%Y-%m')
   latest_date = sales_df['date'].max()


   # èšåˆç‚º SKU Ã— æœˆç¶­åº¦ + B2C ç¯©é¸
   # sales_df = sales_df[sales_df['channel_type'] == 'B2C']
   sales_df['month'] = sales_df['date'].dt.to_period('M')
   monthly_sales = sales_df.groupby(['sku', 'month'])['quantity_sold'].sum().reset_index()
   monthly_sales['date'] = monthly_sales['month'].dt.to_timestamp()


   # åˆä½µç”¢å“è³‡æ–™
   full_df = monthly_sales.merge(product_df, on='sku', how='inner')


   # é æ¸¬å…¬å¼
   def weighted_average(group, weights):
       # æ’é™¤ç•¶æœˆè³‡æ–™ä»¥å…ä½ä¼°
       historical = group[group['date'] < latest_date]
       recent_1m = historical[historical['date'] == latest_date - pd.DateOffset(months=1)]['quantity_sold'].sum()
       recent_3m = historical[historical['date'] >= latest_date - pd.DateOffset(months=3)]['quantity_sold'].mean()
       recent_6m = historical[historical['date'] >= latest_date - pd.DateOffset(months=6)]['quantity_sold'].mean()
       return weights[0] * recent_1m + weights[1] * recent_3m + weights[2] * recent_6m
  # === å›æ¸¬å‡½æ•¸ ===
   def backtest(df, weights, split_date):
      train = df[df['date'] < split_date].copy()
      test = df[df['date'] >= split_date].copy()
        
      predictions = train.groupby('sku').apply(lambda x: weighted_average(x, weights)).reset_index(name='prediction')
        
      merged = test.merge(predictions, on='sku', how='left').fillna(0)  # è™•ç†æ²’æœ‰é æ¸¬å€¼çš„ SKU
      rmse = np.sqrt(mean_squared_error(merged['quantity_sold'], merged['prediction']))
      return rmse

   # === æ¬Šé‡çµ„åˆ ===
   weight_combinations = [
      (0.5, 0.3, 0.2),
      (0.6, 0.2, 0.2),
      (0.4, 0.4, 0.2),
      (0.3, 0.3, 0.4),
      (0.7, 0.2, 0.1),
      (0.1, 0.1, 0.8) # å¢åŠ å¤šçµ„æ¬Šé‡
    ]

   # === è¨­å®šè¨“ç·´é›†å’Œæ¸¬è©¦é›†åˆ†å‰²æ—¥æœŸ ===
   latest_date = sales_df['date'].max()
   split_date = latest_date - pd.DateOffset(months=6)  # ä¾‹å¦‚ï¼Œæœ€å¾Œ 6 å€‹æœˆä½œç‚ºæ¸¬è©¦é›†

   # === è¿´åœˆè¨ˆç®—ä¸¦è©•ä¼° ===
   results = []
   for weights in weight_combinations:
       rmse = backtest(full_df, weights, split_date)
       results.append({'weights': weights, 'rmse': rmse})

   results_df = pd.DataFrame(results)
   best_weights = results_df.loc[results_df['rmse'].idxmin()]['weights']  # æ‰¾åˆ° RMSE æœ€å°çš„æ¬Šé‡çµ„åˆ

   print("å›æ¸¬çµæœ:")
   print(results_df)
   print("\næœ€ä½³æ¬Šé‡:", best_weights)

   # === ä½¿ç”¨æœ€ä½³æ¬Šé‡é€²è¡Œæœ€çµ‚é æ¸¬ ===
   forecast_df = full_df.groupby('sku').apply(lambda x: weighted_average(x, best_weights)).reset_index(name='base_forecast')

   # Debug: æŸ¥çœ‹ç‰¹å®š SKU é æ¸¬ä¾†æºç´°ç¯€
   sku_to_check = 'BLK-P0001'
   sku_group = full_df[full_df['sku'] == sku_to_check].copy()
   print(f"=== â± {sku_to_check} åŸå§‹è¿‘ 6 å€‹æœˆå½™ç¸½éŠ·å”® ===")
   print(sku_group.sort_values('date')[['date', 'quantity_sold']].tail(6))


   historical = sku_group[sku_group['date'] < latest_date]
   q = historical['quantity_sold']
   recent_1m = q[historical['date'] == latest_date - pd.DateOffset(months=1)].sum()
   recent_3m = q[historical['date'] >= latest_date - pd.DateOffset(months=3)].mean()
   recent_6m = q[historical['date'] >= latest_date - pd.DateOffset(months=6)].mean()


   weighted = 0.5 * recent_1m + 0.3 * recent_3m + 0.2 * recent_6m
   print(f"ğŸ” {sku_to_check} åŠ æ¬Šè¨ˆç®—ç´°ç¯€ï¼š")
   print(f" è¿‘1å€‹æœˆç¸½å’Œ: {recent_1m:.2f}")
   print(f" è¿‘3å€‹æœˆå¹³å‡: {recent_3m:.2f}")
   print(f" è¿‘6å€‹æœˆå¹³å‡: {recent_6m:.2f}")
   print(f" â¡ï¸ é æ¸¬åŠ æ¬Šå€¼ï¼ˆæœªèª¿æ•´ï¼‰: {weighted:.2f}")
   forecast_df = forecast_df.merge(product_df[['sku', 'gross_margin', 'product_line', 'type']], on='sku', how='left')


   def adjust_margin(row):
       if pd.isna(row['base_forecast']) or pd.isna(row['gross_margin']):
           return 0
       elif row['gross_margin'] > 0.6:
           return row['base_forecast'] * 1.2
       elif row['gross_margin'] > 0.3:
           return row['base_forecast'] * 1.1
       else:
           return row['base_forecast']


   forecast_df['adjusted_forecast'] = forecast_df.apply(adjust_margin, axis=1).fillna(0).round().astype(int)


   # fallbackï¼šæ–°å“è£œæ¨
   forecast_df['sku'] = forecast_df['sku'].fillna('')
   product_df['sku'] = product_df['sku'].fillna('')
   known_skus = set(forecast_df['sku'].dropna())
   all_skus = set(product_df['sku'].dropna())
   new_skus = list(all_skus - known_skus)


   new_sku_df = product_df[product_df['sku'].isin(new_skus)].copy()
   category_avg = forecast_df.groupby('product_line')['adjusted_forecast'].mean().to_dict()
   new_sku_df['adjusted_forecast'] = new_sku_df['product_line'].map(category_avg).fillna(10).round().astype(int)


   # ä¸‹ä¸€å€‹æœˆæ¨™ç±¤
   next_month = datetime.today().replace(day=1) + pd.DateOffset(months=1)
   date_label = next_month.strftime('%Y-%m')
   forecast_df[date_label] = forecast_df['adjusted_forecast']
   new_sku_df[date_label] = new_sku_df['adjusted_forecast']


   # åˆä½µ + è¼¸å‡ºä¸»é æ¸¬è¡¨
   final_df = pd.concat([forecast_df, new_sku_df])[['sku', 'type', 'product_line', date_label]]
   write_to_gsheet(final_df, SHEET_ID, SHEET_NAME)


   # QA ç¨½æ ¸è¡¨
   all_forecast = pd.concat([forecast_df, new_sku_df]).copy()
   all_forecast['fallback_used'] = all_forecast['sku'].isin(new_sku_df['sku'])
   all_forecast['forecast_type'] = all_forecast['fallback_used'].map(lambda x: 'fallback' if x else 'historical')
   qa_export = all_forecast[['sku', 'product_line', date_label, 'forecast_type']].copy()
   qa_export = qa_export.rename(columns={date_label: 'forecast_value'})
   write_to_gsheet(qa_export, SHEET_ID, 'qa_audit')


   # é æ¸¬é‚è¼¯èªªæ˜è¡¨
   model_description_df = pd.DataFrame({
       'é …ç›®': ['é æ¸¬é‚è¼¯', 'ä½¿ç”¨ç‰¹å¾µ', 'ç‰¹æ®Šè¦å‰‡', 'è³‡æ–™è™•ç†', 'é æ¸¬æœŸé–“', 'æ•¸æ“šä¾†æº'],
       'èªªæ˜': [
           'åŠ æ¬Šç§»å‹•å¹³å‡æ³•ï¼ˆè¿‘1æœˆ60%ï¼Œè¿‘3æœˆ30%ï¼Œè¿‘6æœˆ10%ï¼‰',
           'sku, quantity_sold, gross_margin',
           'æ¯›åˆ©ç‡èª¿æ•´ï¼ˆé«˜+20%ã€ä¸­+10%ã€ä½ä¸è®Šï¼‰',
           'éæ¿¾ B2C + æœˆå½™ç¸½ + fallback æ–°å“å¹³å‡',
           f'é æ¸¬æœˆä»½ï¼š{date_label}',
           'APIï¼šsales_history, product_info'
       ]
   })
   write_to_gsheet(model_description_df, SHEET_ID, MODEL_DESCRIPTION_SHEET_NAME)


   send_slack_message(f"âœ… [é æ¸¬ç³»çµ±] å·²å®Œæˆ {date_label} é æ¸¬ï¼Œä¸»è¡¨èˆ‡ QA å ±è¡¨å·²å¯«å…¥ Google Sheet")


except Exception as e:
   print(f"Error: {e}")
   send_slack_message(f"ğŸš¨ [é æ¸¬ç³»çµ±] åŸ·è¡ŒéŒ¯èª¤ï¼š{str(e)}")
